#!/usr/bin/env python3
"""
æµ‹è¯• Qwen3 æ¨¡å‹å¾®è°ƒç¯å¢ƒ
ä¸“æ³¨äº 4bit é‡åŒ–å’Œæ··åˆç²¾åº¦æ”¯æŒ
"""

import torch
import transformers
import peft
import accelerate
import bitsandbytes as bnb
import datasets
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import gc

def test_core_dependencies():
    """æµ‹è¯•æ ¸å¿ƒä¾èµ–"""
    print("=" * 60)
    print("æµ‹è¯•æ ¸å¿ƒä¾èµ–")
    print("=" * 60)
    
    print(f"âœ… PyTorch: {torch.__version__}")
    print(f"âœ… CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"âœ… GPU: {torch.cuda.get_device_name()}")
        print(f"âœ… GPU å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    print(f"âœ… Transformers: {transformers.__version__}")
    print(f"âœ… PEFT: {peft.__version__}")
    print(f"âœ… Accelerate: {accelerate.__version__}")
    print(f"âœ… BitsAndBytes: {bnb.__version__}")
    print(f"âœ… Datasets: {datasets.__version__}")
    print()

def test_4bit_quantization():
    """æµ‹è¯• 4bit é‡åŒ–é…ç½®"""
    print("=" * 60)
    print("æµ‹è¯• 4bit é‡åŒ–é…ç½®")
    print("=" * 60)
    
    try:
        # 4bit é‡åŒ–é…ç½®
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",  # 4-bit NormalFloat
            bnb_4bit_compute_dtype=torch.float16,  # è®¡ç®—æ—¶ä½¿ç”¨ FP16
            bnb_4bit_use_double_quant=True,  # åŒé‡é‡åŒ–èŠ‚çœå†…å­˜
        )
        
        print("âœ… 4bit é‡åŒ–é…ç½®åˆ›å»ºæˆåŠŸ")
        print(f"   - é‡åŒ–ç±»å‹: {bnb_config.bnb_4bit_quant_type}")
        print(f"   - è®¡ç®—ç²¾åº¦: {bnb_config.bnb_4bit_compute_dtype}")
        print(f"   - åŒé‡é‡åŒ–: {bnb_config.bnb_4bit_use_double_quant}")
        
        return bnb_config
    except Exception as e:
        print(f"âŒ 4bit é‡åŒ–é…ç½®å¤±è´¥: {e}")
        return None

def test_mixed_precision():
    """æµ‹è¯•æ··åˆç²¾åº¦æ”¯æŒ"""
    print("=" * 60)
    print("æµ‹è¯•æ··åˆç²¾åº¦æ”¯æŒ")
    print("=" * 60)
    
    # æµ‹è¯• FP16
    try:
        if torch.cuda.is_available():
            x_fp16 = torch.randn(100, 100, dtype=torch.float16, device='cuda')
            y_fp16 = torch.matmul(x_fp16, x_fp16)
            print("âœ… FP16 è®¡ç®—æ”¯æŒ")
        else:
            print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œè·³è¿‡ FP16 æµ‹è¯•")
    except Exception as e:
        print(f"âŒ FP16 æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯• BF16 (å¦‚æœ GPU æ”¯æŒ)
    try:
        if torch.cuda.is_available() and torch.cuda.is_bf16_supported():
            x_bf16 = torch.randn(100, 100, dtype=torch.bfloat16, device='cuda')
            y_bf16 = torch.matmul(x_bf16, x_bf16)
            print("âœ… BF16 è®¡ç®—æ”¯æŒ")
        else:
            print("âš ï¸  BF16 ä¸æ”¯æŒæˆ– CUDA ä¸å¯ç”¨")
    except Exception as e:
        print(f"âŒ BF16 æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•è‡ªåŠ¨æ··åˆç²¾åº¦
    try:
        if torch.cuda.is_available():
            with torch.cuda.amp.autocast(dtype=torch.float16):
                x = torch.randn(100, 100, device='cuda')
                y = torch.matmul(x, x)
            print("âœ… è‡ªåŠ¨æ··åˆç²¾åº¦æ”¯æŒ")
        else:
            print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œè·³è¿‡è‡ªåŠ¨æ··åˆç²¾åº¦æµ‹è¯•")
    except Exception as e:
        print(f"âŒ è‡ªåŠ¨æ··åˆç²¾åº¦æµ‹è¯•å¤±è´¥: {e}")
    print()

def test_lora_config():
    """æµ‹è¯• LoRA é…ç½®"""
    print("=" * 60)
    print("æµ‹è¯• LoRA é…ç½®")
    print("=" * 60)
    
    try:
        # LoRA é…ç½® - é€‚åˆ Qwen æ¨¡å‹
        lora_config = LoraConfig(
            r=16,  # LoRA çŸ©é˜µçš„ç§©
            lora_alpha=32,  # LoRA ç¼©æ”¾å› å­
            target_modules=[  # ç›®æ ‡æ¨¡å—ï¼Œé’ˆå¯¹ Qwen æ¶æ„
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ],
            lora_dropout=0.05,  # Dropout æ¦‚ç‡
            bias="none",  # ä¸è®­ç»ƒåç½®
            task_type="CAUSAL_LM",  # å› æœè¯­è¨€æ¨¡å‹ä»»åŠ¡
        )
        
        print("âœ… LoRA é…ç½®åˆ›å»ºæˆåŠŸ")
        print(f"   - ç§© (r): {lora_config.r}")
        print(f"   - ç¼©æ”¾å› å­ (alpha): {lora_config.lora_alpha}")
        print(f"   - ç›®æ ‡æ¨¡å—: {', '.join(lora_config.target_modules)}")
        print(f"   - Dropout: {lora_config.lora_dropout}")
        
        return lora_config
    except Exception as e:
        print(f"âŒ LoRA é…ç½®å¤±è´¥: {e}")
        return None

def test_model_preparation():
    """æµ‹è¯•æ¨¡å‹å‡†å¤‡ï¼ˆä¸å®é™…åŠ è½½æ¨¡å‹ï¼‰"""
    print("=" * 60)
    print("æµ‹è¯•æ¨¡å‹å‡†å¤‡æµç¨‹")
    print("=" * 60)
    
    try:
        # æ¨¡æ‹Ÿæ¨¡å‹å‡†å¤‡æ­¥éª¤
        print("âœ… æ¨¡å‹å‡†å¤‡æµç¨‹éªŒè¯:")
        print("   1. åˆ›å»º 4bit é‡åŒ–é…ç½®")
        print("   2. åŠ è½½ Qwen3 æ¨¡å‹ (æ¨¡æ‹Ÿ)")
        print("   3. å‡†å¤‡æ¨¡å‹ç”¨äº kbit è®­ç»ƒ")
        print("   4. åº”ç”¨ LoRA é€‚é…å™¨")
        print("   5. é…ç½®è®­ç»ƒå‚æ•°")
        
        # æ¨¡æ‹Ÿè®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir="./qwen3-finetuned",
            per_device_train_batch_size=2,
            gradient_accumulation_steps=4,
            learning_rate=2e-4,
            num_train_epochs=3,
            logging_steps=10,
            save_steps=50,
            save_total_limit=2,
            fp16=True,  # ä½¿ç”¨ FP16 æ··åˆç²¾åº¦
            bf16=False,  # ä¸ä½¿ç”¨ BF16
            optim="paged_adamw_8bit",  # 8bit Adam ä¼˜åŒ–å™¨
            logging_dir="./logs",
            remove_unused_columns=False,
        )
        
        print("âœ… è®­ç»ƒå‚æ•°é…ç½®æˆåŠŸ")
        print(f"   - æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}")
        print(f"   - å­¦ä¹ ç‡: {training_args.learning_rate}")
        print(f"   - FP16: {training_args.fp16}")
        print(f"   - ä¼˜åŒ–å™¨: {training_args.optim}")
        
        return training_args
    except Exception as e:
        print(f"âŒ æ¨¡å‹å‡†å¤‡å¤±è´¥: {e}")
        return None

def test_memory_estimation():
    """ä¼°ç®—å†…å­˜ä½¿ç”¨"""
    print("=" * 60)
    print("å†…å­˜ä½¿ç”¨ä¼°ç®—")
    print("=" * 60)
    
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        available_memory = torch.cuda.memory_allocated(0) / 1024**3
        free_memory = total_memory - available_memory
        
        print(f"âœ… GPU æ€»å†…å­˜: {total_memory:.1f} GB")
        print(f"âœ… å·²ç”¨å†…å­˜: {available_memory:.1f} GB")
        print(f"âœ… å¯ç”¨å†…å­˜: {free_memory:.1f} GB")
        
        # ä¼°ç®— 4bit Qwen3 æ¨¡å‹å†…å­˜éœ€æ±‚
        model_sizes = {
            "Qwen3-1.5B": "1.5 GB (4bit)",
            "Qwen3-4B": "4 GB (4bit)", 
            "Qwen3-7B": "7 GB (4bit)",
            "Qwen3-14B": "14 GB (4bit)",
            "Qwen3-32B": "32 GB (4bit)",
        }
        
        print("\nğŸ“Š æ¨¡å‹å†…å­˜éœ€æ±‚ä¼°ç®— (4bit é‡åŒ–):")
        for model, memory in model_sizes.items():
            size_gb = float(memory.split()[0])
            if size_gb <= free_memory * 0.8:  # ä¿ç•™ 20% ç¼“å†²
                print(f"   âœ… {model}: {memory} (é€‚åˆ)")
            else:
                print(f"   âŒ {model}: {memory} (å†…å­˜ä¸è¶³)")
    else:
        print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œæ— æ³•ä¼°ç®— GPU å†…å­˜")
    print()

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Qwen3 æ¨¡å‹å¾®è°ƒç¯å¢ƒæµ‹è¯•")
    print("ğŸ“‹ ç›®æ ‡: 4bit é‡åŒ– + æ··åˆç²¾åº¦ + LoRA å¾®è°ƒ")
    print()
    
    # æµ‹è¯•æ ¸å¿ƒä¾èµ–
    test_core_dependencies()
    
    # æµ‹è¯• 4bit é‡åŒ–
    bnb_config = test_4bit_quantization()
    
    # æµ‹è¯•æ··åˆç²¾åº¦
    test_mixed_precision()
    
    # æµ‹è¯• LoRA é…ç½®
    lora_config = test_lora_config()
    
    # æµ‹è¯•æ¨¡å‹å‡†å¤‡
    training_args = test_model_preparation()
    
    # å†…å­˜ä¼°ç®—
    test_memory_estimation()
    
    print("=" * 60)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    if all([bnb_config, lora_config, training_args]):
        print("ğŸ‰ ç¯å¢ƒé…ç½®æˆåŠŸï¼æ”¯æŒä»¥ä¸‹åŠŸèƒ½:")
        print("   âœ… 4bit é‡åŒ– (NF4)")
        print("   âœ… æ··åˆç²¾åº¦è®­ç»ƒ (FP16)")
        print("   âœ… LoRA å‚æ•°é«˜æ•ˆå¾®è°ƒ")
        print("   âœ… Qwen3 æ¨¡å‹æ”¯æŒ")
        print("   âœ… å†…å­˜ä¼˜åŒ–è®­ç»ƒ")
        print("\nğŸš€ ç¯å¢ƒå·²å‡†å¤‡å¥½è¿›è¡Œ Qwen3 æ¨¡å‹å¾®è°ƒï¼")
        
        print("\nğŸ’¡ å»ºè®®çš„å¾®è°ƒæµç¨‹:")
        print("   1. å‡†å¤‡æ•°æ®é›†")
        print("   2. åŠ è½½ Qwen3 æ¨¡å‹ (4bit)")
        print("   3. é…ç½® LoRA é€‚é…å™¨")
        print("   4. è®¾ç½®æ··åˆç²¾åº¦è®­ç»ƒ")
        print("   5. å¼€å§‹å¾®è°ƒè®­ç»ƒ")
        print("   6. ä¿å­˜å’Œæµ‹è¯•å¾®è°ƒæ¨¡å‹")
        
    else:
        print("âš ï¸  éƒ¨åˆ†é…ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    print("\nğŸ“ ä¸‹ä¸€æ­¥:")
    print("   - å‡†å¤‡è®­ç»ƒæ•°æ®")
    print("   - ä¸‹è½½ Qwen3 æ¨¡å‹")
    print("   - å¼€å§‹å¾®è°ƒå®éªŒ")

if __name__ == "__main__":
    main()
