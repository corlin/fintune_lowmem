#!/usr/bin/env python3
"""
Qwen3 æ¨¡å‹å¾®è°ƒç¤ºä¾‹
ä½¿ç”¨ 4bit é‡åŒ– + æ··åˆç²¾åº¦ + LoRA å¾®è°ƒ
"""

import torch
import transformers
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
import numpy as np
import os
#Qwen/Qwen3-4B-Thinking-2507
#Qwen/Qwen3-4B-Instruct-2507
def setup_model_and_tokenizer(model_name="Qwen/Qwen3-4B-Thinking-2507"):
    """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨ - é’ˆå¯¹æ˜¾å­˜æ·±åº¦ä¼˜åŒ–"""
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
    
    # æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    
    # 4bit é‡åŒ–é…ç½® - é’ˆå¯¹16Gæ˜¾å­˜æ·±åº¦ä¼˜åŒ–
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",  # 4-bit NormalFloatï¼Œæœ€ä½³ç²¾åº¦
        bnb_4bit_compute_dtype=torch.bfloat16,  # è®¡ç®—æ—¶ä½¿ç”¨ BF16ï¼Œç²¾åº¦æ›´å¥½
        bnb_4bit_use_double_quant=True,  # åŒé‡é‡åŒ–èŠ‚çœæ˜¾å­˜
        llm_int8_threshold=6.0,
        
        # é¢å¤–çš„é‡åŒ–ä¼˜åŒ–
        bnb_4bit_storage_dtype=torch.uint8,  # å­˜å‚¨æ—¶ä½¿ç”¨8ä½
        load_in_8bit=False,  # ä¸ä½¿ç”¨8bité‡åŒ–ï¼Œ4bitæ›´èŠ‚çœæ˜¾å­˜
        
        # æ–°å¢OOMé˜²æŠ¤å‚æ•°
        bnb_4bit_quant_storage=torch.uint8,  # é‡åŒ–å­˜å‚¨ä½¿ç”¨uint8
        quant_method="bitsandbytes",  # æ˜ç¡®æŒ‡å®šé‡åŒ–æ–¹æ³•
    )
    
    # åŠ è½½æ¨¡å‹ - æ·»åŠ OOMé˜²æŠ¤
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",  # è‡ªåŠ¨è®¾å¤‡æ˜ å°„
        trust_remote_code=True,
        low_cpu_mem_usage=True,  # ä½CPUå†…å­˜ä½¿ç”¨
        torch_dtype=torch.bfloat16,  # æŒ‡å®šæ•°æ®ç±»å‹
    )
    
    # å‡†å¤‡æ¨¡å‹ç”¨äº kbit è®­ç»ƒ - æ·»åŠ OOMé˜²æŠ¤
    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
    )
    
    # è®¾ç½® pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print("âœ… æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½å®Œæˆ")
    return model, tokenizer

def setup_lora(model):
    """è®¾ç½® LoRA é€‚é…å™¨ - é’ˆå¯¹æ˜¾å­˜æ·±åº¦ä¼˜åŒ–"""
    print("ğŸ”„ æ­£åœ¨é…ç½® LoRA é€‚é…å™¨ï¼ˆæ·±åº¦æ˜¾å­˜ä¼˜åŒ–ç‰ˆï¼‰")
    
    # æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # LoRA é…ç½® - é’ˆå¯¹16Gæ˜¾å­˜æ·±åº¦ä¼˜åŒ–
    lora_config = LoraConfig(
        r=6,  # è¿›ä¸€æ­¥å‡å°LoRAçŸ©é˜µçš„ç§©ä»¥èŠ‚çœæ˜¾å­˜ï¼ˆä»8é™åˆ°4ï¼‰
        lora_alpha=12,  # ç›¸åº”è°ƒæ•´ç¼©æ”¾å› å­ï¼ˆalpha = 2*rï¼‰
        target_modules=[  # ç²¾é€‰ç›®æ ‡æ¨¡å—ï¼Œæœ€å°åŒ–æ˜¾å­˜å ç”¨
            "q_proj", "v_proj", "ffn.w1"  # åªå¯¹æœ€é‡è¦çš„æ³¨æ„åŠ›æ¨¡å—åº”ç”¨LoRA
        ],
        lora_dropout=0.1,  # é™ä½dropoutä»¥å‡å°‘è®¡ç®—å¼€é”€
        bias="none",  # ä¸è®­ç»ƒåç½®ä»¥èŠ‚çœå‚æ•°
        task_type="CAUSAL_LM",  # å› æœè¯­è¨€æ¨¡å‹ä»»åŠ¡
        
        # é¢å¤–çš„ä¼˜åŒ–å‚æ•°
        modules_to_save=[],  # ä¸ä¿å­˜é¢å¤–æ¨¡å—ä»¥èŠ‚çœæ˜¾å­˜
        use_rslora=True,  # ä½¿ç”¨Rank-Stabilized LoRAæé«˜è®­ç»ƒç¨³å®šæ€§
        loftq_config=None,  # ä¸ä½¿ç”¨LoFTQä»¥èŠ‚çœè®¡ç®—èµ„æº
        
        # æ–°å¢OOMé˜²æŠ¤å‚æ•°
        init_lora_weights="gaussian",  # ä½¿ç”¨é«˜æ–¯åˆå§‹åŒ–
        fan_in_fan_out=False,  # ç¦ç”¨fan-in fan-outä»¥èŠ‚çœè®¡ç®—
    )
    
    # åº”ç”¨ LoRA é€‚é…å™¨ - æ·»åŠ OOMé˜²æŠ¤
    model = get_peft_model(model, lora_config)
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜
    if hasattr(model, "enable_input_require_grads"):
        model.enable_input_require_grads()
    
    # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
    model.train()
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    trainable_percentage = 100 * trainable_params / total_params
    
    print(f"âœ… LoRA é…ç½®å®Œæˆ")
    print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"   - æ€»å‚æ•°: {total_params:,}")
    print(f"   - å¯è®­ç»ƒæ¯”ä¾‹: {trainable_percentage:.2f}%")
    
    return model

def load_qa_data_from_files():
    """ä» data ç›®å½•åŠ è½½ QA æ•°æ®"""
    print("ğŸ”„ æ­£åœ¨ä» data ç›®å½•åŠ è½½ QA æ•°æ®")
    
    qa_data = []
    
    # å®šä¹‰æ•°æ®æ–‡ä»¶è·¯å¾„
    data_files = [
        "data/raw/39786qa.md",
        "data/raw/qA.md"
    ]
    
    for file_path in data_files:
        if os.path.exists(file_path):
            print(f"ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶: {file_path}")
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # è§£æ QA æ•°æ®
                if file_path == "data/raw/39786qa.md":
                    # è§£æ 39786qa.md æ ¼å¼ (Q1: ... A1: ...)
                    import re
                    qa_pairs = re.findall(r'Q(\d+):\s*(.*?)\s*A\1:\s*(.*?)(?=Q\d+:|$)', content, re.DOTALL)
                    for q_num, question, answer in qa_pairs:
                        question = question.strip()
                        answer = answer.strip()
                        if question and answer:
                            qa_data.append({
                                "instruction": question,
                                "input": "",
                                "output": answer
                            })
                elif file_path == "data/raw/qA.md":
                    # è§£æ qA.md æ ¼å¼ (### Q1: ... A1: ...)
                    import re
                    qa_pairs = re.findall(r'###\s*Q(\d+):\s*(.*?)\s*A\1:\s*(.*?)(?=###\s*Q\d+:|$)', content, re.DOTALL)
                    for q_num, question, answer in qa_pairs:
                        question = question.strip()
                        answer = answer.strip()
                        if question and answer:
                            qa_data.append({
                                "instruction": question,
                                "input": "",
                                "output": answer
                            })
                
                print(f"âœ… ä» {file_path} æˆåŠŸè§£æ {len([q for q in qa_pairs if q[1].strip() and q[2].strip()])} æ¡ QA æ•°æ®")
                
            except Exception as e:
                print(f"âš ï¸  è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        else:
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    print(f"ğŸ“Š æ€»å…±åŠ è½½äº† {len(qa_data)} æ¡ QA æ•°æ®")
    return qa_data

def prepare_dataset(tokenizer, dataset_path=None, max_length=256):
    """å‡†å¤‡è®­ç»ƒæ•°æ®é›† - é’ˆå¯¹å†…å­˜æ·±åº¦ä¼˜åŒ–"""
    print("ğŸ”„ æ­£åœ¨å‡†å¤‡æ•°æ®é›†ï¼ˆæ·±åº¦å†…å­˜ä¼˜åŒ–ç‰ˆï¼‰")
    
    # æ¸…ç†å†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # ä¼˜å…ˆä» data ç›®å½•åŠ è½½ QA æ•°æ®
    qa_data = load_qa_data_from_files()
    
    if qa_data:
        # ä½¿ç”¨åŠ è½½çš„ QA æ•°æ®
        from datasets import Dataset
        dataset = Dataset.from_list(qa_data)
        print("âœ… ä½¿ç”¨ä» data ç›®å½•åŠ è½½çš„ QA æ•°æ®")
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° QA æ•°æ®ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®
        print("âš ï¸  æœªæ‰¾åˆ° QA æ•°æ®ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
        sample_data = [
            {
                "instruction": "ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
                "input": "",
                "output": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦åˆ†æ”¯ï¼Œåˆ›å»ºæ‰§è¡Œäººç±»æ™ºèƒ½ä»»åŠ¡çš„ç³»ç»Ÿã€‚"
            },
            {
                "instruction": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
                "input": "",
                "output": "æœºå™¨å­¦ä¹ æ˜¯AIå­é¢†åŸŸï¼Œè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›ã€‚"
            },
            {
                "instruction": "è§£é‡Šæ·±åº¦å­¦ä¹ ",
                "input": "",
                "output": "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘ï¼Œè‡ªåŠ¨å­¦ä¹ ç‰¹å¾è¡¨ç¤ºã€‚"
            },
            {
                "instruction": "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ",
                "input": "",
                "output": "ç¥ç»ç½‘ç»œæ˜¯æ¨¡ä»¿ç”Ÿç‰©ç¥ç»ç½‘ç»œçš„æ•°å­¦æ¨¡å‹ï¼Œé€šè¿‡è°ƒæ•´æƒé‡å­¦ä¹ å…³ç³»ã€‚"
            },
            {
                "instruction": "ç®€è¿°è‡ªç„¶è¯­è¨€å¤„ç†",
                "input": "",
                "output": "NLPæ˜¯AIåˆ†æ”¯ï¼Œä¸“æ³¨è®¡ç®—æœºä¸äººç±»è¯­è¨€äº¤äº’ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†æå’Œæœºå™¨ç¿»è¯‘ã€‚"
            }
        ]
        
        # è½¬æ¢ä¸º Hugging Face æ•°æ®é›†æ ¼å¼
        from datasets import Dataset
        dataset = Dataset.from_list(sample_data)
    
    # å¦‚æœæä¾›äº†å¤–éƒ¨æ•°æ®é›†è·¯å¾„ï¼Œåˆ™åŠ è½½å¤–éƒ¨æ•°æ®é›†
    if dataset_path is not None:
        external_dataset = load_dataset(dataset_path, split="train")
        dataset = dataset.concatenate(external_dataset)
        print(f"âœ… åˆå¹¶å¤–éƒ¨æ•°æ®é›†ï¼Œæ€»å…± {len(dataset)} æ¡æ ·æœ¬")
    
    # æ ¼å¼åŒ–æ•°æ®ä¸º Qwen æ ¼å¼ - ç®€åŒ–æ ¼å¼ä»¥å‡å°‘tokenæ•°é‡
    def format_example(example):
        """æ ¼å¼åŒ–å•ä¸ªç¤ºä¾‹"""
        instruction = example["instruction"]
        input_text = example["input"]
        output = example["output"]
        
        # æ„å»ºç®€åŒ–çš„ Qwen æ ¼å¼å¯¹è¯
        if input_text:
            prompt = f"ç”¨æˆ·: {instruction} {input_text}\nåŠ©æ‰‹: {output}"
        else:
            prompt = f"ç”¨æˆ·: {instruction}\nåŠ©æ‰‹: {output}"
        
        return {"text": prompt}
    
    # åº”ç”¨æ ¼å¼åŒ–
    formatted_dataset = dataset.map(format_example)
    
    # åˆ†è¯å‡½æ•° - æ·±åº¦ä¼˜åŒ–å†…å­˜ä½¿ç”¨
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding=False,  # ä¸åœ¨åˆ†è¯æ—¶å¡«å……ï¼Œåœ¨æ•°æ®æ•´ç†å™¨ä¸­å¤„ç†
            max_length=max_length,  # è¿›ä¸€æ­¥å‡å°æœ€å¤§é•¿åº¦ä»¥èŠ‚çœæ˜¾å­˜ï¼ˆä»384é™åˆ°256ï¼‰
            return_tensors=None
        )
    
    # åº”ç”¨åˆ†è¯ - ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡å¤§å°ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
    tokenized_dataset = formatted_dataset.map(
        tokenize_function,
        batched=True,
        batch_size=4,  # è¿›ä¸€æ­¥å‡å°æ‰¹æ¬¡å¤§å°ä»¥èŠ‚çœå†…å­˜
        remove_columns=formatted_dataset.column_names,
        num_proc=1,  # ä½¿ç”¨å•è¿›ç¨‹ä»¥é¿å…å†…å­˜é—®é¢˜
    )
    
    # è¿‡æ»¤è¿‡çŸ­çš„æ ·æœ¬ä»¥æé«˜è®­ç»ƒè´¨é‡
    def filter_short_samples(example):
        """è¿‡æ»¤è¿‡çŸ­çš„æ ·æœ¬"""
        return len(example["input_ids"]) >= 16  # æœ€å°é•¿åº¦ä¸º16ä¸ªtoken
    
    filtered_dataset = tokenized_dataset.filter(filter_short_samples)
    
    print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼Œå…± {len(filtered_dataset)} æ¡æ ·æœ¬ï¼ˆè¿‡æ»¤åï¼‰")
    return filtered_dataset

def setup_training_args(output_dir="./qwen3-finetuned"):
    """è®¾ç½®è®­ç»ƒå‚æ•° - é’ˆå¯¹16Gæ˜¾å­˜æ·±åº¦ä¼˜åŒ–"""
    print("ğŸ”„ æ­£åœ¨é…ç½®è®­ç»ƒå‚æ•°ï¼ˆæ·±åº¦æ˜¾å­˜ä¼˜åŒ–ç‰ˆï¼‰")
    
    # æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    training_args = TrainingArguments(
        # åŸºæœ¬å‚æ•° - æ·±åº¦ä¼˜åŒ–æ‰¹æ¬¡å¤§å°ä»¥é€‚åº”16Gæ˜¾å­˜
        output_dir=output_dir,
        per_device_train_batch_size=4,  # æœ€å°æ‰¹æ¬¡å¤§å°ä»¥èŠ‚çœæ˜¾å­˜
        gradient_accumulation_steps=16,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ä»¥ä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°
        learning_rate=5e-5,  # è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡ä»¥æé«˜ç¨³å®šæ€§
        num_train_epochs=100,  # å¢åŠ è®­ç»ƒè½®æ¬¡ä»¥æå‡æ¨¡å‹æ•ˆæœï¼ˆä»2è½®å¢åŠ åˆ°5è½®ï¼‰
        
        # ä¼˜åŒ–å™¨å‚æ•° - ä½¿ç”¨æœ€å†…å­˜é«˜æ•ˆçš„ä¼˜åŒ–å™¨
        optim="paged_adamw_8bit",  # 8bit Adam ä¼˜åŒ–å™¨ï¼ŒèŠ‚çœæ˜¾å­˜
        weight_decay=0.01,
        warmup_ratio=0.2,  # å¢åŠ é¢„çƒ­æ¯”ä¾‹ä»¥æé«˜ç¨³å®šæ€§
        
        # æ··åˆç²¾åº¦ - ä¼˜å…ˆä½¿ç”¨BF16å¦‚æœæ”¯æŒï¼Œå¦åˆ™ä½¿ç”¨FP16
        fp16=False,  # å…³é—­FP16
        bf16=True,   # ä¼˜å…ˆä½¿ç”¨BF16ï¼Œç²¾åº¦æ›´å¥½ä¸”åœ¨æŸäº›GPUä¸Šæ›´å¿«
        
        # ä¿å­˜å’Œæ—¥å¿— - å¤§å¹…å‡å°‘ä¿å­˜é¢‘ç‡ä»¥å‡å°‘IOå¼€é”€å’Œæ˜¾å­˜ä½¿ç”¨
        logging_steps=10,  # è¿›ä¸€æ­¥å‡å°‘æ—¥å¿—é¢‘ç‡
        save_steps=10,    # è¿›ä¸€æ­¥å‡å°‘ä¿å­˜é¢‘ç‡
        save_total_limit=2,  # åªä¿ç•™æœ€æ–°çš„1ä¸ªæ£€æŸ¥ç‚¹ä»¥èŠ‚çœç£ç›˜ç©ºé—´
        logging_dir="./logs",
        
        # è¯„ä¼°å‚æ•°
        eval_strategy="no",
        
        # å…¶ä»–å‚æ•°
        report_to="tensorboard",
        run_name="qwen3-lora-finetuning-deep-optimized",
        
        # å†…å­˜ä¼˜åŒ– - å¯ç”¨æ‰€æœ‰å¯ç”¨çš„å†…å­˜ä¼˜åŒ–æŠ€æœ¯
        gradient_checkpointing=True,  # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ŒèŠ‚çœæ˜¾å­˜ä½†å¢åŠ è®¡ç®—æ—¶é—´
        ddp_find_unused_parameters=False,
        
        # æ•°æ®åŠ è½½ä¼˜åŒ– - è¿›ä¸€æ­¥ä¼˜åŒ–ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
        dataloader_num_workers=4,  # å‡å°‘å·¥ä½œçº¿ç¨‹ä»¥èŠ‚çœå†…å­˜
        dataloader_pin_memory=True,  # å›ºå®šå†…å­˜ä»¥åŠ é€Ÿæ•°æ®ä¼ è¾“
        
        # å†…å­˜å’Œæ€§èƒ½ä¼˜åŒ–
        per_device_eval_batch_size=2,  # è¯„ä¼°æ—¶ä¹Ÿä½¿ç”¨å°æ‰¹æ¬¡
        max_grad_norm=0.5,  # é™ä½æ¢¯åº¦è£å‰ªé˜ˆå€¼ä»¥æé«˜ç¨³å®šæ€§
        seed=42,  # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯é‡å¤æ€§
        
        # é¢å¤–çš„å†…å­˜ä¼˜åŒ–
        fp16_opt_level="O1",  # æ··åˆç²¾åº¦ä¼˜åŒ–çº§åˆ«
        tf32=True,  # å¦‚æœGPUæ”¯æŒï¼Œä½¿ç”¨TF32ç²¾åº¦
        
        # æ–°å¢OOMé˜²æŠ¤å‚æ•°
        dataloader_drop_last=True,  # ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡ä»¥é¿å…å†…å­˜é—®é¢˜
        remove_unused_columns=True,  # ç§»é™¤æœªä½¿ç”¨çš„åˆ—ä»¥èŠ‚çœå†…å­˜
        push_to_hub=False,  # ä¸æ¨é€åˆ°hubä»¥èŠ‚çœç½‘ç»œå’Œå†…å­˜
        local_rank=-1,  # å•GPUè®­ç»ƒ
        fp16_full_eval=False,  # è¯„ä¼°æ—¶ä¸ä½¿ç”¨FP16ä»¥èŠ‚çœæ˜¾å­˜
    )
    
    print("âœ… è®­ç»ƒå‚æ•°é…ç½®å®Œæˆï¼ˆæ·±åº¦æ˜¾å­˜ä¼˜åŒ–ç‰ˆï¼‰")
    return training_args

def train_model(model, tokenizer, tokenized_dataset, training_args):
    """è®­ç»ƒæ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹")
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # ä¸ä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        processing_class=tokenizer,  # ä½¿ç”¨ processing_class æ›¿ä»£å·²å¼ƒç”¨çš„ tokenizer
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ“Š è®­ç»ƒå¼€å§‹...")
    trainer.train()
    
    print("âœ… è®­ç»ƒå®Œæˆ")
    return trainer

def save_model(trainer, output_dir="./qwen3-finetuned"):
    """ä¿å­˜æ¨¡å‹"""
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹åˆ°: {output_dir}")
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model(output_dir)
    
    # ä¿å­˜åˆ†è¯å™¨ - ä½¿ç”¨ processing_class æ›¿ä»£å·²å¼ƒç”¨çš„ tokenizer
    if hasattr(trainer, 'processing_class') and trainer.processing_class is not None:
        trainer.processing_class.save_pretrained(output_dir)
    elif hasattr(trainer, 'tokenizer') and trainer.tokenizer is not None:
        trainer.tokenizer.save_pretrained(output_dir)
    else:
        print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°åˆ†è¯å™¨ï¼Œè·³è¿‡åˆ†è¯å™¨ä¿å­˜")
    
    print(f"âœ… æ¨¡å‹ä¿å­˜å®Œæˆ: {output_dir}")

def test_inference(model_path, test_prompt="å¯†é’¥ç®¡ç†æ˜¯ä»€ä¹ˆ"):
    """æµ‹è¯•æ¨¡å‹æ¨ç†"""
    print("ğŸ§ª æ­£åœ¨æµ‹è¯•æ¨¡å‹æ¨ç†")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    #Qwen/Qwen3-4B-Thinking-2507
    #Qwen/Qwen3-4B-Instruct-2507
    base_model_name = "Qwen/Qwen3-4B-Thinking-2507"
    
    # 4bit é‡åŒ–é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True,
    )
    
    # åŠ è½½PEFTé€‚é…å™¨
    from peft import PeftModel
    model = PeftModel.from_pretrained(model, model_path)
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # å‡†å¤‡è¾“å…¥
    prompt = f"<|im_start|>user\n{test_prompt}<|im_end|>\n<|im_start|>assistant\n"
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # ç§»åŠ¨åˆ° GPU
    if torch.cuda.is_available():
        inputs = {k: v.to("cuda") for k, v in inputs.items()}
        model.to("cuda")
    
    # ç”Ÿæˆå›ç­”
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=1024,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # è§£ç è¾“å‡º
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print(f"ğŸ“ æµ‹è¯•ç»“æœ:")
    print(f"é—®é¢˜: {test_prompt}")
    print(f"å›ç­”: {response}")
    
    return response

def monitor_gpu_memory():
    """ç›‘æ§GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ“Š GPUæ˜¾å­˜ä½¿ç”¨: {allocated:.1f}GB å·²åˆ†é…, {cached:.1f}GB å·²ç¼“å­˜, æ€»å…± {total:.1f}GB")
        return allocated, cached, total
    return 0, 0, 0

def cleanup_gpu_memory():
    """æ¸…ç†GPUå†…å­˜ä»¥é¿å…OOM"""
    if torch.cuda.is_available():
        print("ğŸ§¹ æ¸…ç†GPUå†…å­˜...")
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        import gc
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("âœ… GPUå†…å­˜æ¸…ç†å®Œæˆ")

def check_memory_safety(required_memory_gb=2.0):
    """æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ˜¾å­˜ç»§ç»­è®­ç»ƒ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        available = total - allocated
        
        if available < required_memory_gb:
            print(f"âš ï¸  è­¦å‘Š: å¯ç”¨æ˜¾å­˜ä¸è¶³ ({available:.1f}GB < {required_memory_gb}GB)")
            cleanup_gpu_memory()
            # å†æ¬¡æ£€æŸ¥
            allocated = torch.cuda.memory_allocated() / 1024**3
            available = total - allocated
            if available < required_memory_gb:
                print(f"âŒ é”™è¯¯: æ˜¾å­˜ä»ç„¶ä¸è¶³ ({available:.1f}GB < {required_memory_gb}GB)")
                return False
        return True
    return True  # CPUæ¨¡å¼ä¸‹æ€»æ˜¯è¿”å›True

def main():
    """ä¸»å‡½æ•° - é’ˆå¯¹16Gæ˜¾å­˜æ·±åº¦ä¼˜åŒ–ç‰ˆæœ¬"""
    print("ğŸš€ Qwen3 æ¨¡å‹å¾®è°ƒç¤ºä¾‹ï¼ˆ16Gæ˜¾å­˜æ·±åº¦ä¼˜åŒ–ç‰ˆï¼‰")
    print("ğŸ“‹ åŠŸèƒ½: 4bit é‡åŒ– + BF16 + LoRA + æ¢¯åº¦ç´¯ç§¯ + OOMé˜²æŠ¤")
    print("ğŸ¯ ç›®æ ‡: åœ¨16Gæ˜¾å­˜é™åˆ¶ä¸‹å®‰å…¨ç¨³å®šåœ°å®Œæˆè®­ç»ƒ")
    print()
    
    # æ£€æŸ¥ GPU
    if torch.cuda.is_available():
        print(f"âœ… GPU å¯ç”¨: {torch.cuda.get_device_name()}")
        print(f"âœ… GPU å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        
        # è®¾ç½®GPUä¼˜åŒ–
        cleanup_gpu_memory()
        torch.backends.cudnn.benchmark = True  # å¯ç”¨cuDNNåŸºå‡†æµ‹è¯•
        torch.backends.cuda.matmul.allow_tf32 = True  # å…è®¸TF32çŸ©é˜µä¹˜æ³•
        print("âœ… GPU ä¼˜åŒ–å·²å¯ç”¨")
    else:
        print("âš ï¸  GPU ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU è®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
    
    print()
    
    # åˆå§‹æ˜¾å­˜ç›‘æ§å’Œæ£€æŸ¥
    monitor_gpu_memory()
    if not check_memory_safety(required_memory_gb=3.0):
        print("âŒ åˆå§‹æ˜¾å­˜æ£€æŸ¥å¤±è´¥ï¼Œæ— æ³•å¼€å§‹è®­ç»ƒ")
        return
    
    print()
    
    try:
        # 1. è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨ - æ·»åŠ OOMé˜²æŠ¤
        print("ğŸ“‹ æ­¥éª¤ 1: åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨")
        #Qwen/Qwen3-4B-Thinking-2507
        model_name = "Qwen/Qwen3-4B-Thinking-2507"#"Qwen/Qwen3-4B-Instruct-2507"
        model, tokenizer = setup_model_and_tokenizer(model_name)
        
        # æ£€æŸ¥æ˜¾å­˜
        monitor_gpu_memory()
        if not check_memory_safety(required_memory_gb=2.5):
            print("âŒ æ¨¡å‹åŠ è½½åæ˜¾å­˜ä¸è¶³")
            return
        
        print()
        
        # 2. è®¾ç½® LoRA - æ·»åŠ OOMé˜²æŠ¤
        print("ğŸ“‹ æ­¥éª¤ 2: é…ç½® LoRA é€‚é…å™¨")
        model = setup_lora(model)
        
        # æ£€æŸ¥æ˜¾å­˜
        monitor_gpu_memory()
        if not check_memory_safety(required_memory_gb=2.0):
            print("âŒ LoRAé…ç½®åæ˜¾å­˜ä¸è¶³")
            return
        
        print()
        
        # 3. å‡†å¤‡æ•°æ®é›† - æ·»åŠ OOMé˜²æŠ¤
        print("ğŸ“‹ æ­¥éª¤ 3: å‡†å¤‡æ•°æ®é›†")
        tokenized_dataset = prepare_dataset(tokenizer)
        
        # æ¸…ç†å†…å­˜å¹¶æ£€æŸ¥
        cleanup_gpu_memory()
        monitor_gpu_memory()
        if not check_memory_safety(required_memory_gb=1.5):
            print("âŒ æ•°æ®é›†å‡†å¤‡åæ˜¾å­˜ä¸è¶³")
            return
        
        print()
        
        # 4. è®¾ç½®è®­ç»ƒå‚æ•°
        print("ğŸ“‹ æ­¥éª¤ 4: é…ç½®è®­ç»ƒå‚æ•°")
        training_args = setup_training_args()
        
        print()
        
        # 5. è®­ç»ƒæ¨¡å‹ - æ·»åŠ OOMé˜²æŠ¤
        print("ğŸ“‹ æ­¥éª¤ 5: å¼€å§‹è®­ç»ƒæ¨¡å‹")
        trainer = train_model(model, tokenizer, tokenized_dataset, training_args)
        
        print()
        
        # 6. ä¿å­˜æ¨¡å‹ - æ·»åŠ OOMé˜²æŠ¤
        print("ğŸ“‹ æ­¥éª¤ 6: ä¿å­˜æ¨¡å‹")
        cleanup_gpu_memory()
        save_model(trainer)
        
        print()
        
        # 7. æµ‹è¯•æ¨ç† - æ·»åŠ OOMé˜²æŠ¤
        print("ğŸ“‹ æ­¥éª¤ 7: æµ‹è¯•æ¨¡å‹æ¨ç†")
        cleanup_gpu_memory()
        test_inference("./qwen3-finetuned")
        
        print("\nğŸ‰ å¾®è°ƒå®Œæˆï¼")
        print("ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: ./qwen3-finetuned")
        print("ğŸ“Š è®­ç»ƒæ—¥å¿—åœ¨: ./logs")
        
        # æœ€ç»ˆæ˜¾å­˜çŠ¶æ€
        print("\nğŸ“Š æœ€ç»ˆæ˜¾å­˜çŠ¶æ€:")
        monitor_gpu_memory()
        
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print("âŒ CUDA OOM é”™è¯¯: æ˜¾å­˜ä¸è¶³")
            print("ğŸ’¡ å»ºè®®:")
            print("   1. é‡å¯ç¨‹åºé‡Šæ”¾æ˜¾å­˜")
            print("   2. è¿›ä¸€æ­¥å‡å°æ‰¹æ¬¡å¤§å°æˆ–åºåˆ—é•¿åº¦")
            print("   3. è€ƒè™‘ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
            cleanup_gpu_memory()
        else:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æœ€ç»ˆæ¸…ç†
        print("\nğŸ§¹ æ‰§è¡Œæœ€ç»ˆæ¸…ç†...")
        cleanup_gpu_memory()
        print("âœ… æ¸…ç†å®Œæˆ")

if __name__ == "__main__":
    main()
